\section{Cloud Grading Architecture With OpenEdX}
\label{sec:arch}

We adopt a narrow Unix-like view of an autograder: it is a
stateless command-line program that, given a student work submission and
a rubric, computes a score and some textual feedback.  We treat
separately the question of how to connect this program to an LMS.
All other policy issues---whether students can resubmit homeworks, how
late penalties are computes, where the gradebook is stored, any other
LMS behaviors---are out of scope, as is the question of whether these
autograders should replace or supplement manual grading by instructors.

\subsection{Student Experience}

Our initial implementation of \texttt{rag} was designed to work with
Coursera, but both the interface and student experience are similar
between Coursera and OpenEdX.  A logged-in learner navigates to a page
containing instructions and handouts for a homework assignment;
when ready, the learner submits a single file or a
\texttt{tar} or \texttt{zip} archive through a standard HTTP
file-upload form.  A short time later, typically less than a minute, the
learner can refresh the page to see feedback on her work from the
autograder.  Depending on the course policy, she may be able to improve
her work and resubmit.

\subsection{Interface Between Autograders and OpenEdX}

\begin{figure}
  \centering
  \includegraphics[width=0.75\textwidth]{figs/autograder_arch.pdf}
  \caption{\label{fig:arch}
  Since our autograders rely on many libraries,
  tools, support files, and so on, we encapsulate the autograder in a
  virtual machine image that is deployed on Amazon Elastic Compute Cloud.
  When a new instance is started, the autograder script automatically runs from
  \texttt{/etc/init.d} and examines a deploy-time environment variable
  to get obtain the credentials needed to make calls to the XQueues.}
\end{figure}

When an OpenEdX student uploads an assignment that is to be graded by an
external grader, the student's submitted file, along with additional
submission metadata specified at course authoring time, goes into a
persistent named queue; each course has its own queue.
We use the submission metadata field to distinguish different
assignments so that the autograder knows which engine and rubric files
must be used to grade that assignment.
The OpenEdX LMS defines a simple RESTful
API~\uf{edx-partner-course-staff.readthedocs.org/en/latest/exercises\_tools/external\_graders.html}
by which external standalone autograders can retrieve student
submissions from these queues and later post back a numerical grade and
textual feedback.

%% Retrieving an
%% assignment and posting back a 
%% grade are separate queue operations, to allow for autograders that may
%% take a long time to run.  
%% If the autograder polls the XQueue and finds it empty, the grader
%% process sleeps for awhile and tries again.  (In the future we will allow
%% idle autograders to kill themselves.)
While simple, this architecture exposes several important properties
that simplify the creation of such external graders.
First, if an assignment is retrieved but no grade is posted back before
a pre-set timeout, OpenEdX eventually returns the ungraded assignment to
the queue, where it will presumably be picked up again by another
autograder instance.
Therefore, if an autograder crashes while grading an assignment, no
student work is lost.
Second, since the autograders themselves are stateless consumers
contacting a single producer (the queue), and grading is
embarrassingly task-parallel, we can drain the queues faster by simply
deploying additional autograder instances.
Since we package the entire autograder and supporting libraries as a
virtual machine image deployed on Amazon's cloud, deploying an
additional grader is essentially a one-click operation. (We have not
yet had the time to automate scaling and provisioning.)
\footnote{OpenEdX also supports an alternative ``push'' protocol in which each student
submission event triggers a call to a RESTful autograder endpoint.
We do not use this
alternative protocol because it thwarts this simple scaling technique
and because we would be  unable to limit the rate at which
submissions were pushed to our autograders during peak times.}
Even our most sophisticated autograders take less than one
machine-minute per assignment, so at less than 10 cents per machine-hour,
MOOC-scale 
autograding is cost-effective and fast: even with
thousands of assignments being submitted in a 50,000-learner course,
students rarely waited more than a few minutes to get feedback.

If the external grader crashes, it can simply be restarted, which we in
fact do in the body of a \texttt{while(true)} shell script.  If the
entire VM becomes unresponsive, for example if it becomes corrupted by
misbehaving student code, it can be rebooted or undeployed as
needed. 

The external grader does not have access to the identity of the learner;
instead, an obfuscated token identifies the learner, with the mappings
to the learners' true identities maintained only on OpenEdX.
Hence no sensitive information connecting a work product to a specific
student is leaked if the autograder is compromised.

\subsection{Rubric Files}

autograder, rubric file

current refactoring to allow rubric files to be pulled on demand and
updated in place

\subsection{CI Workflow for Autograders}

To ensure that changes to rubric files, homework assignments, or the
autograder code itself doesn't break the autograders, 
we setup continuous integration tasks using Travis-CI, which is integrated with
GitHub and runs on submission of patches for acceptance (``pull request''
in GitHub terminology), as well as other events. Users develop on their
own forks of a repository, then submit pull requests back to the main
repository when finished. Even when not finished, pull requests to the
main repo may draw attention that can keep the team informed or resolve
issues. 

When a build is triggered, it typically loads a new virtual host, installs runtime binaries and basic libraries, clones the associated GitHub repo including the .travis.yml configuration file, installs dependencies, and runs indicated scripts. Our simple configuration file only specifies a Ruby version before Travis calls the commands under the 'script:' key, which start the test suites. Those commands use 'bundle exec' to choose rspec and cucumber versions indicated in the project's Gemfile. The tests are in the repo's spec and features folders, and their output is included in the record of the build run.

\begin{figure}[!htbp]
  \centering
  \begin{minipage}{0.70\textwidth}%
  \lstset{tabsize=1,basicstyle=\scriptsize\ttfamily}%
  \lstinputlisting{figs/travis.yml}%
  \end{minipage}
  \caption{\label{fig:rag-ci}%
  Typical .travis.yml file.
}
\end{figure}

For the autograders themselves, it runs the rspec and cucumber tests located in the default spec and features directories. For the separate homework repositories that use the autograders as a library, we used cucumber Scenario Outline AST tables to organize tabular input and assert results. These CI features run the autograders locally against inputs via the Open3 library in order to control output streams and return codes. In the homework repos, we also added a cucumber feature to script the install and verify the configuration of the autograders, which simplified the use of multiple repositories, especially for new users.

\begin{figure}[!htbp]
  \centering
  \begin{minipage}{0.99\textwidth}%
  \lstset{tabsize=1,basicstyle=\scriptsize\ttfamily}%
  \lstinputlisting{figs/ci_feature.txt}%
  \end{minipage}
  \caption{\label{fig:rag-ci}%
  Cucumber feature Scenario Outline AST table for CI.
}
\end{figure}


\subsection{Robustness and Security}

\tbd{finish this section}

Since ill-behaved student code is a concern, there are various layers of
security.  For the RSpecGrader, the tests must be run in the same
process as the student's code.  They are run in a separate interpreter
thread that is protected by a timeout and in which large 

Multiple layers of security: watchdog timers, sandboxed interpreter, threads. Hollingsworth 1960 observed that it was possible for students to submit programs that deliberately damage the autograder.


Sandboxing: while our RSpec-based autograder performs much of its
  own sandboxing, even if student code escapes the sandbox we can shut
  down and undeploy the VM.  




