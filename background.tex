\section{Introduction and Background}

Automated assessment of student programming assignments was first tried
over fifty years ago~\cite{hollingsworth60}, and with the arrival of
Massive Open Online Courses (MOOCs), so-called ``autograding'' is
receiving renewed attention.  The appeal is obvious: students not only
get immediate feedback, but can now be given multiple opportunities to
resubmit their code to improve on their mistakes, providing the
opportunity for mastery learning.  Over their long history, autograders
have evolved from test-harness libraries that must be linked against
student code to web-based systems that perform both dynamic tests and static
analysis~\cite{douce-2005-autograding-survey}.  Autograders have also
found use in residential classrooms, with some instructors even finding
that frequent autograded programming assignments are a surprisingly good
predictor of final course grades~\cite{navrat2014}.

We describe our experience developing and using a specific category of
cloud-based autograder in conjunction with both a software engineering
MOOC and the residential course on which the MOOC is based.  To
establish our position in the landscape, our autograder is \emph{fully
automatic}, rather than serving to increase instructor productivity
during manual grading.  It is also \emph{test based}, relying on actual
execution of student code under controlled conditions to determine
correctness, in contrast to graders that rely on static analysis or on
comparing the output of student code with a reference solution.

In this section we give some brief background about the course for which
the autograders were built and the rationale for building them in the
first place.

\subsection{Course: Engineering Software as a Service (ESaaS)}

From 2008 to 2010, authors Fox and Patterson thoroughly revised
UC~Berkeley's undergraduate software engineering
course~\cite{crossing_the_software_chasm,agile_sw_curriculum} to bring
it up-to-date with modern practices.  The course focuses on agile
development with an emphasis on
behavior-driven design (BDD)~\cite{bdd} and automated testing.  A key
design philosophy of the course was that \emph{methodologies become
tools:\/} rather than simply advising students to use a certain
methodology (for example, test-first development or behavior-driven
design), we would provide them with a best-of-breed tool to immediately
practice that methodology.  These tools would not only enable the
students to learn immediately by doing, but also provide quantitative
feedback for instructors to check students' work.  Among the tools we
use~\footnote{Web
sites: \texttt{rspec.info}, \texttt{github.com/colszowka/simplecov}, \texttt{cukes.info}, \texttt{pivotaltracker.com}, \texttt{travis-ci.org}, \texttt{codeclimate.com}, \texttt{github.com}.} 
are 
RSpec for unit testing; 
SimpleCov for C0 test coverage; 
Cucumber for behavior-driven design and integration testing; 
Pivotal Tracker for lightweight project management;
Travis for continuous-integration testing; 
CodeClimate for code-quality metrics;
and, of course, GitHub at the center, with which all the
other tools communicate.  All are either open source downloads or offer a free
hosted version sufficient for class projects.

We chose Ruby on Rails as the teaching vehicle because its developer
ecosystem has by far the richest set of such tools, with a much stronger
emphasis on high productivity, refactoring, and beautiful code than any
other ecosystem we'd seen.
The choice of Rails influenced our decision to make Software as a
Service (SaaS) the learning vehicle, rather than (for example) mobile
client apps.

The new course was offered experimentally in 2009--2010 and was
immediately successful; growing enrollment demand led 
us to write a book around the course~\cite{esaaS} and to start thinking
about how to offer it to more students by automating some of the
grading.  Coincidentally, in mid-2011 our colleagues Prof.~Andrew Ng
and Prof.~Daphne Koller at Stanford were experimenting with a MOOC
platform which would eventually become Coursera, and invited us to try
adapting part of our course to the MOOC 
platform as an experiment.  With the help of some very strong teaching
assistants, we not only created Berkeley's first MOOC, but also the
initial versions of the autograder technology described here.  To date,
we estimate over 1,500 engineer-hours have been invested in the
autograders, including contributions from MOOC alumni and from
instructors using our MOOC materials as a SPOC~\cite{moocs-spocs-TR}.

about autograding.


about ESaaS; projects; use of Pivotal Tracker, CodeClimate, Travis for coverage

initially on Coursera, then edX.  Coursera opened up an API for us. edX
opened a simlar one later. Details of the latter in Section~\ref{sec:arch}.

\subsection{Why Another Autograder?}

Given that 17 autograding systems and over 60 papers about them were
produced from 2006--2010 alone~\cite{ihantola-2010-autograding-survey},
why did we choose to build our own?

First, as the survey authors point
out~\cite{ihantola-2010-autograding-survey}, many existing systems' code
is not readily available or is tightly integrated to a particular Learning
Management System (LMS).  We would need to integrate with Coursera and
later OpenEdX, both of which were new and had not at that time embraced
any education-technology interoperability standards such as
LTI\uf{imsglobal.org/lti}.  Unlike most previous
systems, ours would need to work at ``cloud scale'' and respond to
workload spikes---the initial
offering of our MOOC attracted over 50,000 learners, and we expected
that thousands of submissions would arrive bunched together close to the
submission deadline.  For the same reason, our graders needed to be
highly insulated from the LMS, so that students whose code accidentally
or deliberately damaged the autograder could not compromise other
information in the LMS.
Similarly, with such a large number of international learners (less than
25\% of MOOC learners were from the USA) with diverse hardware,
software, and operating systems, autograding had to be ``zero
configuration,'' requiring no installation of software on learners' own computers.

Finally, it is widely acknowledged that the Ruby and Rails
ecosystem has developed by far the richest set of tools for testing,
code grooming, and so on.
Indeed, this was a major reason we chose Ruby for our course,
relying on these tools both to help the students learn software
engineering processes and to help instructors quantify students' mastery.
We believed we could repurpose those same tools into autograders that
would give finer-grained feedback, could perform more detailed tests,
and would be easier to repurpose than those built for other languages.
