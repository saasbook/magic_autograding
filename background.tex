\section{Background: Autograding for a Software Engineering Course}

Automated assessment of student programming assignments was first tried
over fifty years ago~\cite{hollingsworth60}, and with the arrival of
Massive Open Online Courses (MOOCs), so-called ``autograders'' are
receiving renewed attention.  The appeal is obvious: students not only
get immediate feedback, but can now be given multiple opportunities to
resubmit their code to improve on their mistakes, providing the
opportunity for mastery learning.  Over their long history, autograders
have evolved from test-harness libraries that must be linked against
student code to web-based systems that perform both dynamic tests and static
analysis~\cite{douce-2005-autograding-survey}.  Autograders have also
found use in residential classrooms, with some instructors even finding
that grades on autograded programming assignments are a surprisingly good
predictor of final course grades~\cite{navrat2014}.

From 2008 to 2010, authors Fox and Patterson refocused
UC~Berkeley's one-semester (14-week) undergraduate software engineering
course~\cite{crossing_the_software_chasm,agile_sw_curriculum} on agile
development, emphasizing
behavior-driven design (BDD)\uf{guide.agilealliance.org/guide/bdd.html}
and automated testing.  A key goal
of the redesign
was to promote software engineering methodologies
by giving students access to best-of-breed tools to immediately
practice those methodologies.  These tools would not only enable the
students to learn immediately by doing, but also provide quantitative
feedback for instructors to check students' work.  
We chose Ruby on Rails as the teaching vehicle because its developer
ecosystem has by far the richest set of such tools, with a much stronger
emphasis on high productivity, refactoring, and beautiful code than any
other ecosystem we'd seen.
The choice of Rails in turn influenced our decision to use Software as a
Service (SaaS) as the learning vehicle, rather than (for example) mobile
or embedded apps.
Among the tools we
use\footnote{All are either open source downloads or offer a free
hosted version sufficient for class projects.  Web
sites: \texttt{rspec.info}, \texttt{github.com/colszowka/simplecov},
\texttt{cukes.info}, \texttt{pivotaltracker.com},
\texttt{travis-ci.org}, \texttt{codeclimate.com}, \texttt{heroku.com}, \texttt{github.com}.} 
are 
RSpec for unit testing,
SimpleCov for C0 test coverage,
Cucumber for behavior-driven design and integration testing,
Pivotal Tracker for lightweight project management,
Travis for continuous-integration testing,
CodeClimate for code-quality metrics,
Heroku for lightweight continuous deployment to the public cloud,
and, of course, GitHub, with which all the
other tools communicate.  
In just 14 weeks, third- and fourth-year students learn Ruby
and Rails (most haven't seen it before), learn the above tools, complete
five programming assignments, take three exams, and form ``two-pizza
teams'' of 4--6 to prototype a real SaaS application for a nonprofit,
NGO, or campus unit, over four two-week agile iterations.

The new course was offered experimentally in 2009--2010 and was
immediately successful; growing enrollment demand (from 45 in the pilot
to 240 in Spring 2015) led 
us to write a book around the course~\cite{esaaS} and to start thinking
about how to scale it up.  
Coincidentally, in mid-2011 our colleagues Prof.~Andrew Ng
and Prof.~Daphne Koller at Stanford were experimenting with a MOOC
platform which would eventually become Coursera, and invited us to try
adapting part of our course to the
platform as an experiment.  With the help of some very strong teaching
assistants, we not only created Berkeley's first MOOC, but also the
initial versions of the autograder technology described here.  To date,
we estimate over 1,500 engineer-hours have been invested in the
autograders, including contributions from MOOC alumni,
from the AgileVentures\uf{agileventures.org}  open development community,
and from
instructors using our MOOC materials as a SPOC~\cite{moocs-spocs-TR}.

