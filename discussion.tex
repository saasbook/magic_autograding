\section{Lessons and Future Work}

Both  surveys of autograders ask why existing autograders aren't reused
more, at least when the programming languages and types of assignments supported by the
autograder match those used in courses other than the one(s) for which
the autograder was designed.
We believe one reason is the configuration required for teachers to
deploy autograders and
students to submit work to them.  Since we
faced and surmounted this problem in deploying our ``autograders as a
service'' with OpenEdX, we can make them easy for others to use.
We already have several instructors running SPOCs based on our
materials~\cite{moocs-spocs-TR} using OpenEdX, not only using our
autograders but creating new assignments that take advantage of them.
We are completing a major refactoring that should allow our autograders
to be used entirely as a service by others, and a toolchain to create
autogradable homeworks for use in conjunction with the ESaaS course
materials. 

We now discuss how we are addressing ongoing
challenges resulting from lessons learned in using these autograders for
nearly three years.

\textbf{Tuning rubrics.}
When rubrics for new assignments are developed, it is easy to overlook
correct implementations that don't match the rubric, and easy to forget
``preflight checks'' that may cause the grader process to give up (for
example, checking that a function is defined in the appropriate class
namespace before calling it, to avoid a ``method not found''
exception).  Similarly, if tests are
redundant---that is, if the same single line of code or few lines of
code in a student submission causes all tests in a group to either pass
or fail together---then student scoring is distorted.  (This is the more
general problem of test suite quality in software engineering.)
In general we try to debug rubrics at classroom scale and
then deploy the assignments to the MOOC, relying on the CI workflow to
ensure we haven't broken the autograding of existing assignments.

\textbf{Avoiding ``Autograder-Driven Development.''}
Because feedback from the autograder is quick, students can get into the
habit of relying on the autograder for debugging.  To some extent we
have turned this into a benefit by showing students how to use
RSpec and Cucumber/Capybara 
on their own computers\footnote{These and all the other tools are preinstalled in the virtual machine
image in which we package all student-facing courseware, available for
download from \url{saasbook.info}.}
and run a subset of the same tests the
instructors use, which is much faster and also gives them access to an
interactive debugger.

\textbf{Combining with manual grading.}  In a classroom setting (though
usually not in a MOOC), instructors may want to
spot-check students' code manually  in addition to having it
autograded.  The current workflow makes it a  bit awkward to do this,
though we do save a copy of every graded assignment.

\textbf{Grading for style.}
As Douce et al.\ observe~\cite{douce-2005-autograding-survey}, one flaw
of many autograders is that ``A program\ldots{}may be correct in its
operation yet pathological in its construction.''
We have observed this problem firsthand and are developing
``crowd-aware'' 
autograders that take advantage of
scale to give students feedback on style as well as correctness.
This work is based
on two main ideas.
The first is that a small number of clusters may capture the
a majority of stylistic errors, and browsing these clusters can help 
the instructor quickly grasp the main categories of stylistic problems students
are experiencing~\cite{clustering-style}.  The second is that within a sufficiently large set of
student submissions, we can observe not only examples of excellent style and
examples of very poor style, but enough examples in between that we can
usually identify a submission that is \emph{slightly} more stylistic
than a given student's submission~\cite{autostyle}.  We can then use the differences
between two submissions as the basis for giving a hint to the student
whose submission is stylistically worse.

\textbf{Cheating.}
Woit and Mason~\cite{woit2003} found that not only is cheating rampant
(in their own 5-year study and supported by earlier studies), as
demonstrated dramatically by students who got high marks on required
programming assignments but failed the exact same questions when they
appeared on exams, but also that students don't do optional exercises.
Notwithstanding these findings---and we're sure plagiarism is occurring
in both our MOOC and campus class---plagiarism detection has been a
non-goal for us.
We use these assignments as formative rather than summative assessments,
and we have the option of using
MOSS\uf{theory.stanford.edu/\textasciitilde{}aiken/moss}.
That said, we continue to work on strengthening the autograders against
common student attacks, such as trying to generate output that mimics
what the autograder generates when outputting a score, with the goal of
getting the synthetic output parsed as the definitive score.

%% We do want to avoid students submitting the URI of a colleague's
%% deployed app to the MechanizeGrader; we are planning to require
%% that each student's app serve pages that include hidden HTML elements
%% whose ID matches the student ID.  Since no way to tell when your app will be
%% polled, hard to cheat.

%% Others (cite) have instrumented every code checkin and/or instrumented the desktop IDE  such as Eclipse (cite); Cloud-based IDEs may allow deeper instrumentation (cite Cryolite).

