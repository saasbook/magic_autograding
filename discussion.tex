\section{Lessons and Challenges}

Both  surveys of autograders ask why existing autograders aren't reused more.
We believe one reason is the configuration required for teachers to deploy them and
students to submit work to them.  Since we
faced and surmounted this problem in deploying our ``autograders as a
service'' with OpenEdX, we can make them easy for others to use.
The simple external grader architecture of OpenEdX provides a good separation
of concerns between the LMS and autograder authors.
We already have several instructors running SPOCs based on our
materials~\cite{moocs-spocs-TR} using OpenEdX, not only using our
autograders but creating new assignments that take advantage of them.
We are completing a major refactoring that should allow our autograders
to be used entirely as a service by others, and a toolchain to create
autogradable homeworks for use in conjunction with the ESaaS course
materials. 

In the rest of this section we discuss how we are addressing ongoing
challenges resulting from lessons learned in using these autograders for
nearly three years.

\textbf{Tuning rubrics.}
When rubrics for new assignments are developed, it is easy to overlook
correct implementations that don't match the rubric, and easy to forget
``preflight checks'' that may cause the grader process to give up (for
example, checking that a function is defined in the appropriate class
namespace before calling it, to avoid a ``method not found''
exception).  In general we try to debug rubrics at classroom scale and
then deploy the assignments to the MOOC, relying on the CI workflow to
ensure we haven't broken the autograding of existing assignments.

\textbf{Test suite quality.}  A general challenge in software
engineering is assessing the quality of a test suite, which is separate
from the concept of coverage.  If multiple tests are effectively
redundant---that is, if the same single line of code or few lines of
code in a student submission causes all tests in a group to either pass
or fail together---then student scoring is distorted.

\textbf{Avoiding ``Autograder-Driven Development.''}
Because feedback from the autograder is quick, students can get into the
habit of relying on the autograder for debugging.  To some extent we
have turned this into a benefit by showing students how to install RSpec
on their own computers and run a subset of the same tests the
instructors use, which (we hope) gets them into the habit of always
testing as they code.

\textbf{Combining with manual grading.}  In a classroom setting (though
usually not in a MOOC),i nstructors may want to
spot-check students' code manually  in addition to having it
autograded.  The current workflow makes it a  bit awkward to do this,
though we do save a copy of every graded assignment.
