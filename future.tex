\section{Future Work}



a.	As Douce et al. note (review), one flaw of many of the autograders in their survey (cite) is that "A program can be submitted that may be correct in its operation yet be pathological in its construction." (give example from Autostyle papers.) Opportuntiy: "rich because of scale" - automatic code feedback based on large # of submissions to same assignment.  CodeWebs, others. pointers to AutoStyle papers.

b.	plagiarism detection: can do, but has generally been a non-goal for us. Woit & Mason found that not only is cheating rampant (in their own 5-year study and supported by earlier studies), demonstrated dramatically by students who got A's on required programming assignments but failed the exact same questions when they appeared on proctored exams, but  also that students don't do optional exercises, and part of the value of an initial online assessment is to serve as a wake-up call to motivate students to practice,

Also can use MOSS.

Plagiarism: how prevent students submitting URI of a colleague's app?
We'll require app to serve pages that include a hidden HTML element
whose ID matches student.  Since no way to tell when your app will be
polled, hard to cheat.


c.	Others (cite) have instrumented every code checkin and/or instrumented the desktop IDE  such as Eclipse (cite); Cloud-based IDEs may allow deeper instrumentation (cite Cryolite).

d.	Octobear integration (trigger autograders from GitHub push)



